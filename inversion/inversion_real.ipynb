{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inversion Notebook"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-07T21:07:42.405429Z",
     "start_time": "2025-01-07T21:07:42.401399Z"
    }
   },
   "source": [
    "import sys\n",
    "import os \n",
    "sys.path.append(os.path.abspath(os.path.join(\"\", \"..\")))\n",
    "import torch\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "from PIL import Image\n",
    "from lora_w2w import LoRAw2w\n",
    "from utils import load_models, inference, save_model_w2w, save_model_for_diffusers\n",
    "from inversion import invert\n",
    "device = \"mps\""
   ],
   "outputs": [],
   "execution_count": 2
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading Base Models"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-07T21:07:58.538922Z",
     "start_time": "2025-01-07T21:07:43.911641Z"
    }
   },
   "source": [
    "unet, vae, text_encoder, tokenizer, noise_scheduler = load_models(device)"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Loading pipeline components...:   0%|          | 0/6 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "d37bc2fe3be44352b26f03b7f51db417"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1) mean is 99648 dimensional tensor of the mean for each parameter in the original LoRA space.\n",
    "2) std is 99648 dimensional tensor of the standard deviation for each parameter in the original LoRA space.\n",
    "3) v is 99648x10000 dimensional tensor used to project or unproject the weights onto principal component representation in w2w space or to unproject back into the LoRA space.\n",
    "4) weight_dimensions is a dictionary of the dimensionality for each LoRA in the UNet. Used to save models in Diffusers pipeline format. "
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-07T21:08:36.239869Z",
     "start_time": "2025-01-07T21:08:32.234575Z"
    }
   },
   "source": [
    "mean = torch.load(\"../files/mean.pt\", map_location=device).bfloat16().to(device)\n",
    "std = torch.load(\"../files/std.pt\", map_location=device).bfloat16().to(device)\n",
    "v = torch.load(\"../files/V.pt\", map_location=device).bfloat16().to(device)\n",
    "weight_dimensions = torch.load(\"../files/weight_dimensions.pt\", map_location=device)"
   ],
   "outputs": [],
   "execution_count": 5
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialize optimization coefficients and load in. 10000 principal coefficients balances editability of the model and identity preservation. "
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-07T21:09:04.338231Z",
     "start_time": "2025-01-07T21:09:04.326069Z"
    }
   },
   "source": [
    "proj = torch.zeros(1,10000).bfloat16().to(device)\n",
    "network = LoRAw2w( proj, mean, std, v[:, :10000], \n",
    "                    unet,\n",
    "                    rank=1,\n",
    "                    multiplier=1.0,\n",
    "                    alpha=27.0,\n",
    "                    train_method=\"xattn-strict\"\n",
    "                ).to(device, torch.bfloat16)\n"
   ],
   "outputs": [],
   "execution_count": 6
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inversion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To run inversion, we provide a given image and an optional mask that can help guide reconstructing identity. You can provide a path to your own image and an optional mask. Refer to the folder structure for the example. If identity reconstruction is still lacking, we suggest increasing the number of epochs to 800."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-07T21:32:18.407992Z",
     "start_time": "2025-01-07T21:18:42.913552Z"
    }
   },
   "source": [
    "network = invert(network=network, unet=unet, vae=vae, text_encoder=text_encoder, tokenizer=tokenizer,\n",
    "                 prompt = \"sks person\", noise_scheduler = noise_scheduler, epochs=400,\n",
    "                 image_path =\"images/real_image/real/test_id\",\n",
    "                 mask_path = None, device = device)"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 400/400 [13:35<00:00,  2.04s/it]\n"
     ]
    }
   ],
   "execution_count": 13
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Show Original Image"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-07T21:42:33.137058Z",
     "start_time": "2025-01-07T21:42:24.183901Z"
    }
   },
   "source": [
    "image = Image.open(\"images/real_image/real/test_id/im/pexels-danxavier-1239291.jpg\")\n",
    "image.show()"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "execution_count": 14
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate samples from inverted identity"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-07T21:44:06.775909Z",
     "start_time": "2025-01-07T21:42:52.701536Z"
    }
   },
   "source": [
    "#inference parameters\n",
    "prompt = \"sks person\" \n",
    "negative_prompt = \"low quality, blurry, unfinished, cartoon\"\n",
    "batch_size = 1\n",
    "height = 512\n",
    "width = 512\n",
    "guidance_scale = 2.5\n",
    "seed = 1\n",
    "ddim_steps = 50\n",
    "# random seed generator\n",
    "generator = torch.Generator(device=device)\n",
    "\n",
    "#run inference\n",
    "image = inference(network, unet, vae, text_encoder, tokenizer, prompt, negative_prompt, guidance_scale, noise_scheduler, ddim_steps, seed, generator, device)\n",
    "\n",
    "### display image\n",
    "image = image.detach().cpu().float().permute(0, 2, 3, 1).numpy()[0]\n",
    "image = Image.fromarray((image * 255).round().astype(\"uint8\"))\n",
    "image.show()"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 51/51 [01:10<00:00,  1.38s/it]\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "execution_count": 15
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save model as a *w2w* model (just the 10000 coefficients)"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-07T21:53:21.256539Z",
     "start_time": "2025-01-07T21:53:21.220727Z"
    }
   },
   "source": [
    "save_model_w2w(network, path=\"../output/real_inversion_w2w\")"
   ],
   "outputs": [],
   "execution_count": 16
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also save it in a format compatible with Diffusers pipelines"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-07T21:53:28.906289Z",
     "start_time": "2025-01-07T21:53:28.338656Z"
    }
   },
   "source": [
    "save_model_for_diffusers(network,std, mean, v, weight_dimensions, path=\"../output/real_inversion_diffusers\")"
   ],
   "outputs": [],
   "execution_count": 17
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dblora",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
